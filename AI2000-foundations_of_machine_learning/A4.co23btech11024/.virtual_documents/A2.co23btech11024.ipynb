import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import sympy as sy
import scipy as sp











n=100 # 100 data points
x=np.random.uniform(0, 1, n)
x.sort()
eps=np.random.normal(0, 0.1, n)

y=np.sin(2*np.pi*x)+eps








dim=25

mu=np.linspace(0,1, dim)
# std_dev_gauss_basis=1/(mu[1]-mu[0])
std_dev_gauss_basis=(mu[1]-mu[0])


def gauss_basis(x, mu, sigma):
    return np.exp(-0.5*(((x-mu)/sigma)**2))


# phi_x=np.array([(gauss_basis(x[j], mu[i], std_dev_gauss_basis)) for i in range(dim)], dtype=np.float32).reshape(1,-1)
# phi_x.shape





beta=1/(0.1**2)

post_mu=[]
post_cov=[]

post_mu.append(np.zeros(dim).reshape(-1,1))
post_cov.append(np.eye(dim))

for j in range(x.shape[0]):
    phi_x=np.array([(gauss_basis(x[j], mu[i], std_dev_gauss_basis)) for i in range(dim)], dtype=np.float32).reshape(1,-1)
    
    dum1=np.linalg.pinv(post_cov[-1])
    dum2=(beta*(phi_x.T@phi_x))
    # print((phi_x.T@phi_x).shape)
    post_cov.append(np.linalg.pinv(dum1+dum2))
    
    dum1=(np.linalg.pinv(post_cov[-2])@post_mu[-1])
    dum2=beta*(phi_x.T@y.reshape(-1,1)[j]).reshape(-1,1)
    post_mu.append(post_cov[-1]@(dum1+dum2))











w_sampled=[]
w_sampled.append(np.random.multivariate_normal(post_mu[-1].squeeze(1), post_cov[-1], 1))

y_sampled=[]
phi_x_sampled=np.array([(gauss_basis(x, mu[i], std_dev_gauss_basis)) for i in range(dim)], dtype=np.float32)
y_sampled.append(w_sampled[-1]@phi_x_sampled)

phi_x_sampled.shape
# w_sampled[-1].shape





m=20
w_sampled.append(np.random.multivariate_normal(post_mu[-1].squeeze(1), post_cov[-1], m-1))
for j in range(m-1):
    y_sampled.append(w_sampled[-1][j].reshape(1,-1)@phi_x_sampled)

y_sampled_arr=np.array(y_sampled).squeeze(1)

plt.plot(x, y)
for i in range(len(y_sampled)):
    plt.plot(x, y_sampled_arr[i])
    plt.xlabel('x')
    plt.ylabel('y')
plt.show()





y_sam=y_sampled_arr.sum(axis=0)/len(y_sampled)
print("Avg y value computed from sampled values:")
print(y_sam)

# plt.scatter(x, np.sin(2*np.pi*x)+eps)
plt.scatter(x, y)
plt.scatter(x, y_sam)
plt.legend(["Actual Sine Curve", "Sampled Avg Sine Curve"])
plt.xlabel('x')
plt.ylabel('y')
plt.show()








n=[5, 20, 100] 


def sequential_bayesian_update(post_mu, post_cov, n1):
    n=n1 # n1 data points
    x=np.random.uniform(0, 1, n)
    eps=np.random.normal(0, 0.1, n)
    
    y=np.sin(2*np.pi*x)+eps

    dim=25
    mu=np.linspace(0,1, dim)
    # std_dev_gauss_basis=1/(mu[1]-mu[0])
    std_dev_gauss_basis=(mu[1]-mu[0])

    beta=1/(0.1**2)
    post_mu=[]
    post_cov=[]
    
    post_mu.append(np.zeros(dim).reshape(-1,1))
    post_cov.append(np.eye(dim))
    
    for j in range(x.shape[0]):
        phi_x=np.array([(gauss_basis(x[j], mu[i], std_dev_gauss_basis)) for i in range(dim)], dtype=np.float32).reshape(1,-1)
        
        dum1=np.linalg.pinv(post_cov[-1])
        dum2=(beta*(phi_x.T@phi_x))
        # print((phi_x.T@phi_x).shape)
        post_cov.append(np.linalg.pinv(dum1+dum2))
        
        dum1=(np.linalg.pinv(post_cov[-2])@post_mu[-1])
        dum2=beta*(phi_x.T@y.reshape(-1,1)[j]).reshape(-1,1)
        post_mu.append(post_cov[-1]@(dum1+dum2))


print(f'Dataset_size: {n[2]}')
x0=np.random.uniform(0, 1, 3)

sequential_bayesian_update(post_mu, post_cov, n[2])
phi_x_random=np.array([(gauss_basis(x0, mu[i], std_dev_gauss_basis)) for i in range(dim)], dtype=np.float32)

pred_mu =[phi_x_random[:,i]@post_mu[-1] for i in range(phi_x_random.shape[1])]
pred_var=[(1/beta)+(phi_x_random[:,i]@post_cov[-1]@(phi_x_random[:,i].T)) for i in range(phi_x_random.shape[1])]

dummy=np.linspace(-2, 2, 200)

for i in range(len(pred_mu)):
    dummy_y=np.exp(-((dummy-pred_mu[i])**2) / (2*pred_var[i])) / np.sqrt(2 * np.pi)
    plt.plot(dummy, dummy_y)
    plt.title(f'Normal_{i+1}: Mean = {pred_mu[i]}, Variance: {pred_var[i]}')
    plt.vlines(pred_mu[i]+(2*(pred_var[i]**0.5)), 0,max(dummy_y), 'r', linestyles='dotted')
    plt.vlines(pred_mu[i]-(2*(pred_var[i]**0.5)), 0,max(dummy_y), 'r', linestyles='dotted')
    # Denoting 95% confidence interval with shaded region
    plt.fill_between(dummy, dummy_y, 0, where=((dummy>=pred_mu[i]-(2*(pred_var[i]**0.5))) & (dummy<=pred_mu[i]+(2*(pred_var[i]**0.5)))))
    plt.ylabel('Probability')
    plt.xlabel('Possible target vals')
    plt.show()





for j in range(len(n)):
    print(f'Dataset_size: {n[j]}')
    x0=np.random.uniform(0, 1, n[j])
    x0.sort()

    sequential_bayesian_update(post_mu, post_cov, n[j])
    
    phi_x_random=np.array([(gauss_basis(x0, mu[i], std_dev_gauss_basis)) for i in range(dim)], dtype=np.float32)
    
    pred_mu =[phi_x_random[:,i]@post_mu[-1] for i in range(phi_x_random.shape[1])]
    pred_var=[(1/beta)+(phi_x_random[:,i]@post_cov[-1]@(phi_x_random[:,i].T)) for i in range(phi_x_random.shape[1])]
    
    dummy=np.linspace(0, 1, 1000)
    pred_mu=np.array([pred_mu]).squeeze(2).squeeze(0)
    pred_var=np.array([pred_var]).squeeze(0)
    
    plt.plot(dummy, np.sin(2*np.pi*dummy), 'g--')
    plt.scatter(x0.reshape(1,-1), pred_mu, marker='*')
    # Denoting 95% confidence interval with shaded region
    plt.fill_between(x0, pred_mu-(2*(pred_var**0.5)), pred_mu+(2*(pred_var**0.5)), alpha=0.5)
    plt.ylabel('t')
    plt.xlabel('x')
    plt.legend(["sin(2πx)", "Predicted mean for random points", "95% Confidence Interval"], bbox_to_anchor=(1.6,1))
    plt.show()














l=100 # 100 independent datasets
n=25 # 25 data points

x_total=np.random.uniform(0, 1, (n, l))
x_total.sort(axis=0)
eps_total=np.random.normal(0, 0.1, (n, l))

y_total=np.array([(np.sin(2*np.pi*x_total[:,i])+eps_total[:,i]) for i in range(x_total.shape[1])])[:, :, None]








dim_2=25

mu_2=np.linspace(0,1, dim_2)
std_dev_gauss_basis_2=mu_2[1]-mu_2[0]


lamb=[1e-4, 10, 1e4]
basis_mat = np.array([(gauss_basis(x_total, mu_2[i], std_dev_gauss_basis_2)) for i in range(dim_2)], dtype=np.float32).T

for i in range(len(lamb)):
    dummy=np.transpose(basis_mat, (0,2,1))@basis_mat
    dummy+=(lamb[i]*(np.zeros((l,dim_2,dim_2))+np.eye(dim_2)))            # Using 3D Identity matrix
    
    # print(basis_mat.shape)
    dummy=(np.linalg.inv(dummy)@np.transpose(basis_mat, (0,2,1)))
    dummy=dummy@y_total
    x_plot=np.random.uniform(0, 1, (250, 1))
    x_plot.sort(axis=0)
    des_x=np.array([(gauss_basis(x_plot, mu_2[i], std_dev_gauss_basis_2)) for i in range(dim_2)], dtype=np.float32)
    des_x=np.transpose(des_x, (2,0,1))
    
    y_reg=(np.transpose(dummy, (0,2,1))@des_x)
    
    
    for ji in range(y_reg.shape[0]):
        plt.plot(x_plot.T.squeeze(0), y_reg[ji].squeeze(0))
    plt.show()
    
    dummy=np.linspace(0, 1, 250)    
    plt.plot(dummy, np.sin(2*np.pi*dummy), 'g--')
    plt.scatter(x_plot.squeeze(1), (y_reg.sum(axis=0)/y_reg.shape[0]).squeeze(0))
    plt.legend(["sin(2πx)", "Mean of Estimated Regression Curves"], bbox_to_anchor=(1.65,1))
    plt.ylabel('y')
    plt.xlabel('x')
    plt.show()














n_tot=500
p_class0=0.5


np.random.multivariate_normal(np.ones(2,), np.eye(2,2), 3)


def datagen(n_tot, p_class0, mean1, mean2, cov1, cov2, noise_bool, mean_noise, var_noise):
    
    # Class Sizes
    n_0 = int(n_tot * p_class0)
    n_1 = n_tot - n_0
    data_0 = np.random.multivariate_normal(mean1, cov1, n_0).T
    data_1 = np.random.multivariate_normal(mean2, cov2, n_1).T
    
    # Adding Gaussian noise if required
    if noise_bool:
        print("Gaussian Noise is added to Data")
        data_0 += np.random.normal(mean_noise, var_noise, (2, n_0))
        data_1 += np.random.normal(mean_noise, var_noise, (2, n_1))
    
    np.random.shuffle(data_0.T)
    np.random.shuffle(data_1.T)
    
    # Using random train and test ratio for each class
    frac_train_0 = np.random.uniform(0.75, 0.85)
    frac_train_1 = np.random.uniform(0.75, 0.85)
    n_train_0 = int(frac_train_0 * n_0)
    n_train_1 = int(frac_train_1 * n_1)
    
    # Fixing 10% of class size as min test set size
    min_test_0 = max(1, int(0.1 * n_0))
    min_test_1 = max(1, int(0.1 * n_1))
    # Adjusting train set sizes if original train set leads to test set with very few entries
    n_train_0 = min(n_train_0, n_0 - min_test_0)
    n_train_1 = min(n_train_1, n_1 - min_test_1)
    
    train_data_0 = data_0[:, :n_train_0]
    test_data_0  = data_0[:, n_train_0:]
    train_data_1 = data_1[:, :n_train_1]
    test_data_1  = data_1[:, n_train_1:]
    
    # plt.scatter(data_0[0, :], data_0[1, :])
    # plt.scatter(data_1[0, :], data_1[1, :])
    # plt.xlabel('x'); plt.ylabel('y')
    # plt.legend(['Class_0', 'Class_1'], bbox_to_anchor=(1.25, 1))
    # plt.show()
    # plt.scatter(train_data_0[0, :], train_data_0[1, :])
    # plt.scatter(test_data_0[0, :], test_data_0[1, :])
    # plt.xlabel('x'); plt.ylabel('y')
    # plt.legend(['Train_Class_0', 'Test_Class_0'], bbox_to_anchor=(1.35, 1))
    # plt.show()
    # plt.scatter(train_data_1[0, :], train_data_1[1, :])
    # plt.scatter(test_data_1[0, :], test_data_1[1, :])
    # plt.xlabel('x'); plt.ylabel('y')
    # plt.legend(['Train_Class_1', 'Test_Class_1'], bbox_to_anchor=(1.35, 1))
    # plt.show()
    
    return train_data_0, test_data_0, train_data_1, test_data_1


train_data_0, test_data_0, train_data_1, test_data_1 = datagen(n_tot=500, p_class0=0.5, mean1=np.array([0,0]), mean2=np.array([1,3]), cov1=np.eye(2), cov2=np.eye(2), noise_bool=0, mean_noise=0, var_noise=1)





# Lets calculate unbiased mean and unbiased covariance matrices

mu_data_0=(train_data_0.sum(axis=1)/train_data_0.shape[1]).reshape(-1,1)
mu_data_1=(train_data_1.sum(axis=1)/train_data_1.shape[1]).reshape(-1,1)

cov_data_0=(1/(train_data_0.shape[1]-1))*(train_data_0-mu_data_0)@(train_data_0-mu_data_0).T
cov_data_1=(1/(train_data_1.shape[1]-1))*(train_data_1-mu_data_1)@(train_data_1-mu_data_1).T


train_data_1.shape


def plot_decision_boundary(X_train, y_train, classifier_func, title):
    h = .05  # step size in the mesh
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Points on the grid to be classified
    # np.c_ concatenates along second axis
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = classifier_func(grid_points)
    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(8, 6))
    cmap_light = ListedColormap(['#FFADAD', '#BDB2FF'])
    cmap_bold = ['#D9534F', '#5D49D1']

    plt.contourf(xx, yy, Z, cmap=cmap_light)
    scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=ListedColormap(cmap_bold), edgecolor='k', s=40)
    plt.title(title, fontsize=16)
    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label='Class 0',
                                  markerfacecolor=cmap_bold[0], markersize=10),
                       plt.Line2D([0], [0], marker='o', color='w', label='Class 1',
                                  markerfacecolor=cmap_bold[1], markersize=10)]
    plt.legend(handles=legend_elements, loc='upper right')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()














def gaussian_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1):

    p_class1=1-p_class0
    
    # Train
    train_res0_0=(1/np.linalg.det(cov_data_0)**0.5)*np.exp(-.5*np.diag((train_data_0-mu_data_0).T@np.linalg.inv(cov_data_0)@(train_data_0-mu_data_0)))*p_class0
    # train_result matrix of train_data_0 wrt class 1
    train_res0_1=(1/np.linalg.det(cov_data_1)**0.5)*np.exp(-.5*np.diag((train_data_0-mu_data_1).T@np.linalg.inv(cov_data_1)@(train_data_0-mu_data_1)))*p_class1
    
    # train_result matrix of train_data_1 wrt class 0
    train_res1_0=(1/np.linalg.det(cov_data_0)**0.5)*np.exp(-.5*np.diag((train_data_1-mu_data_0).T@np.linalg.inv(cov_data_0)@(train_data_1-mu_data_0)))*p_class0
    train_res1_1=(1/np.linalg.det(cov_data_1)**0.5)*np.exp(-.5*np.diag((train_data_1-mu_data_1).T@np.linalg.inv(cov_data_1)@(train_data_1-mu_data_1)))*p_class1
    
    train_res0=[0 if train_res0_0[i]>=train_res0_1[i] else 1 for i in range(train_res0_0.shape[0])]
    train_res1=[0 if train_res1_0[i]>=train_res1_1[i] else 1 for i in range(train_res1_0.shape[0])]

    # Test
    res0_0=(1/np.linalg.det(cov_data_0)**0.5)*np.exp(-.5*np.diag((test_data_0-mu_data_0).T@np.linalg.inv(cov_data_0)@(test_data_0-mu_data_0)))*p_class0
    # Result matrix of test_data_0 wrt class 1
    res0_1=(1/np.linalg.det(cov_data_1)**0.5)*np.exp(-.5*np.diag((test_data_0-mu_data_1).T@np.linalg.inv(cov_data_1)@(test_data_0-mu_data_1)))*p_class1
    
    # Result matrix of test_data_1 wrt class 0
    res1_0=(1/np.linalg.det(cov_data_0)**0.5)*np.exp(-.5*np.diag((test_data_1-mu_data_0).T@np.linalg.inv(cov_data_0)@(test_data_1-mu_data_0)))*p_class0
    res1_1=(1/np.linalg.det(cov_data_1)**0.5)*np.exp(-.5*np.diag((test_data_1-mu_data_1).T@np.linalg.inv(cov_data_1)@(test_data_1-mu_data_1)))*p_class1

    test_res0=[0 if res0_0[i]>=res0_1[i] else 1 for i in range(res0_0.shape[0])]
    test_res1=[0 if res1_0[i]>=res1_1[i] else 1 for i in range(res1_0.shape[0])]

    print("Train Data:")
    print("Class 0 GNB Accuracy: ",1-(sum(train_res0)/len(train_res0)))
    print("Class 1 GNB Accuracy: ",sum(train_res1)/len(train_res1))
    print()
    print("Test Data:")
    print("Class 0 GNB Accuracy: ",1-(sum(test_res0)/len(test_res0)))
    print("Class 1 GNB Accuracy: ",sum(test_res1)/len(test_res1))

    return [1-(sum(train_res0)/len(train_res0)), sum(train_res1)/len(train_res1), 1-(sum(test_res0)/len(test_res0)), sum(test_res1)/len(test_res1)]


gaussian_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)





def bernoulli_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1):
    p_class1=1-p_class0
    # bern_train_0_x=[1 if train_data_0[0,i]>=0 else 0 for i in range(train_data_0.shape[1])]
    # bern_train_0_y=[1 if train_data_0[1,i]>=0 else 0 for i in range(train_data_0.shape[1])]
    # bern_train_1_x=[1 if train_data_1[0,i]>=1 else 0 for i in range(train_data_1.shape[1])]
    # bern_train_1_y=[1 if train_data_1[1,i]>=3 else 0 for i in range(train_data_1.shape[1])]
    
    bern_train_0_x=[1 if train_data_0[0,i]>=mu_data_0[0] else 0 for i in range(train_data_0.shape[1])]
    bern_train_0_y=[1 if train_data_0[1,i]>=mu_data_0[1] else 0 for i in range(train_data_0.shape[1])]
    bern_train_1_x=[1 if train_data_1[0,i]>=mu_data_1[0] else 0 for i in range(train_data_1.shape[1])]
    bern_train_1_y=[1 if train_data_1[1,i]>=mu_data_1[1] else 0 for i in range(train_data_1.shape[1])]
    
    p_0=[(sum(bern_train_0_x)+1)/(len(bern_train_0_x)+2), (sum(bern_train_0_y)+1)/(len(bern_train_0_y)+2)]
    p_1=[(sum(bern_train_1_x)+1)/(len(bern_train_1_x)+2), (sum(bern_train_1_y)+1)/(len(bern_train_1_y)+2)]
    
    print(sum(bern_train_0_x), sum(bern_train_0_y), len(bern_train_0_x))
    print(sum(bern_train_1_x), sum(bern_train_1_y), len(bern_train_1_x))
    
    print(p_0, sum(p_0))
    print(p_1, sum(p_1))
    print()
    
    # bern_test_0_x=[1 if test_data_0[0,i]>=0 else 0 for i in range(test_data_0.shape[1])]
    # bern_test_0_y=[1 if test_data_0[1,i]>=0 else 0 for i in range(test_data_0.shape[1])]
    # bern_test_1_x=[1 if test_data_1[0,i]>=1 else 0 for i in range(test_data_1.shape[1])]
    # bern_test_1_y=[1 if test_data_1[1,i]>=3 else 0 for i in range(test_data_1.shape[1])]
    
    bern_test_0_x=[1 if test_data_0[0,i]>=mu_data_0[0] else 0 for i in range(test_data_0.shape[1])]
    bern_test_0_y=[1 if test_data_0[1,i]>=mu_data_0[1] else 0 for i in range(test_data_0.shape[1])]
    bern_test_1_x=[1 if test_data_1[0,i]>=mu_data_1[0] else 0 for i in range(test_data_1.shape[1])]
    bern_test_1_y=[1 if test_data_1[1,i]>=mu_data_1[1] else 0 for i in range(test_data_1.shape[1])]
    
    test_res_bern_0_0=[]
    test_res_bern_0_1=[]
    test_res_bern_1_0=[]
    test_res_bern_1_1=[]

    train_res_bern_0_0=[]
    train_res_bern_0_1=[]
    train_res_bern_1_0=[]
    train_res_bern_1_1=[]

    # Train
    # Dataset 0
    for i in range(train_data_0.shape[1]):
        # Class 0 predictions
        a= p_0[0] if bern_train_0_x[i]==1 else 1-p_0[0]
        b= p_0[1] if bern_train_0_y[i]==1 else 1-p_0[1]
        train_res_bern_0_0.append(a*b*p_class0)
        
        # Class 1 predictions
        a= p_1[0] if bern_train_0_x[i]==1 else 1-p_1[0]
        b= p_1[1] if bern_train_0_y[i]==1 else 1-p_1[1]
        train_res_bern_0_1.append(a*b*p_class1)
        
    # Dataset 1
    for i in range(train_data_1.shape[1]):
        # Class 0 predictions
        a= p_0[0] if bern_train_1_x[i]==1 else 1-p_0[0]
        b= p_0[1] if bern_train_1_y[i]==1 else 1-p_0[1]
        train_res_bern_1_0.append(a*b*p_class0)
        
        # Class 1 predictions
        a= p_1[0] if bern_train_1_x[i]==1 else 1-p_1[0]
        b= p_1[1] if bern_train_1_y[i]==1 else 1-p_1[1]
        train_res_bern_1_1.append(a*b*p_class1)
    
    # Test
    # Dataset 0
    for i in range(test_data_0.shape[1]):
        # Class 0 predictions
        a= p_0[0] if bern_test_0_x[i]==1 else 1-p_0[0]
        b= p_0[1] if bern_test_0_y[i]==1 else 1-p_0[1]
        test_res_bern_0_0.append(a*b*p_class0)
        
        # Class 1 predictions
        a= p_1[0] if bern_test_0_x[i]==1 else 1-p_1[0]
        b= p_1[1] if bern_test_0_y[i]==1 else 1-p_1[1]
        test_res_bern_0_1.append(a*b*p_class1)
        
    # Dataset 1
    for i in range(test_data_1.shape[1]):
        # Class 0 predictions
        a= p_0[0] if bern_test_1_x[i]==1 else 1-p_0[0]
        b= p_0[1] if bern_test_1_y[i]==1 else 1-p_0[1]
        test_res_bern_1_0.append(a*b*p_class0)
        
        # Class 1 predictions
        a= p_1[0] if bern_test_1_x[i]==1 else 1-p_1[0]
        b= p_1[1] if bern_test_1_y[i]==1 else 1-p_1[1]
        test_res_bern_1_1.append(a*b*p_class1)

    train_res_0=[0 if train_res_bern_0_0[i]>=train_res_bern_0_1[i] else 1 for i in range(len(train_res_bern_0_0))]
    train_res_1=[0 if train_res_bern_1_0[i]>=train_res_bern_1_1[i] else 1 for i in range(len(train_res_bern_1_0))]
    test_res_0=[0 if test_res_bern_0_0[i]>=test_res_bern_0_1[i] else 1 for i in range(len(test_res_bern_0_0))]
    test_res_1=[0 if test_res_bern_1_0[i]>=test_res_bern_1_1[i] else 1 for i in range(len(test_res_bern_1_0))]
    
    print("Train Data:")
    print("Class 0 BNB Accuracy: ",1-(sum(train_res_0)/len(train_res_0)))
    print("Class 1 BNB Accuracy: ",sum(train_res_1)/len(train_res_1))
    print()    
    print("Test Data:")
    print("Class 0 BNB Accuracy: ",1-(sum(test_res_0)/len(test_res_0)))
    print("Class 1 BNB Accuracy: ",sum(test_res_1)/len(test_res_1))

    return [1-(sum(train_res_0)/len(train_res_0)), sum(train_res_1)/len(train_res_1), 1-(sum(test_res_0)/len(test_res_0)), sum(test_res_1)/len(test_res_1)]


bernoulli_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)





def lda_closed_form(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1):
    # Within-Class Scatter Matrix
    s_w =(train_data_0.shape[1]-1)*cov_data_0
    s_w+=((train_data_1.shape[1]-1)*cov_data_1)
    
    # Between Class Scatter Matrix
    mu_tot=(train_data_0.sum(axis=1)+train_data_1.sum(axis=1))/(train_data_0.shape[1]+train_data_1.shape[1])
    s_b =train_data_0.shape[1]*(mu_data_0-mu_tot)@(mu_data_0-mu_tot).T
    s_b+=train_data_1.shape[1]*(mu_data_1-mu_tot)@(mu_data_1-mu_tot).T
    
    eigie=np.linalg.eig(np.linalg.inv(s_w)@s_b)
    thresh_lda=((eigie[1][:,0]@mu_data_0)+(eigie[1][:,0]@mu_data_1))/2

    y_lda_train_0=eigie[1][:,0]@train_data_0
    y_lda_train_1=eigie[1][:,0]@train_data_1
    y_lda_0=eigie[1][:,0]@test_data_0
    y_lda_1=eigie[1][:,0]@test_data_1
    
    train_res_lda0=[0 if y_lda_train_0[i]>=thresh_lda else 1 for i in range(y_lda_train_0.shape[0])]
    train_res_lda1=[0 if y_lda_train_1[i]>=thresh_lda else 1 for i in range(y_lda_train_1.shape[0])]
    test_res_lda0=[0 if y_lda_0[i]>=thresh_lda else 1 for i in range(y_lda_0.shape[0])]
    test_res_lda1=[0 if y_lda_1[i]>=thresh_lda else 1 for i in range(y_lda_1.shape[0])]

    print("Train Data:")
    print("Class 0 LDA Accuracy: ", sum(train_res_lda0)/y_lda_train_0.shape[0])
    print("Class 1 LDA Accuracy: ", 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]))
    print()
    print("Test Data:")
    print("Class 0 LDA Accuracy: ", sum(test_res_lda0)/y_lda_0.shape[0])
    print("Class 1 LDA Accuracy: ", 1-(sum(test_res_lda1)/y_lda_1.shape[0]))

    return eigie[1][:,0], [sum(train_res_lda0)/y_lda_train_0.shape[0], 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]), sum(test_res_lda0)/y_lda_0.shape[0], 1-(sum(test_res_lda1)/y_lda_1.shape[0])]


def lda_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, eta, epochs):
    # Within-Class Scatter Matrix
    s_w =(train_data_0.shape[1]-1)*cov_data_0
    s_w+=((train_data_1.shape[1]-1)*cov_data_1)
    
    # Between Class Scatter Matrix
    mu_tot=(train_data_0.sum(axis=1)+train_data_1.sum(axis=1))/(train_data_0.shape[1]+train_data_1.shape[1])
    s_b =train_data_0.shape[1]*(mu_data_0-mu_tot)@(mu_data_0-mu_tot).T
    s_b+=train_data_1.shape[1]*(mu_data_1-mu_tot)@(mu_data_1-mu_tot).T

    w=np.ones(s_w.shape[0]).reshape(-1,1)

    for i in range(epochs):
        du1=w.T@s_w@w
        du2=w.T@s_b@w
        grad_w=(2*(-du2*s_w+du1*s_b)@w)/(du1**2)
        w-=eta*grad_w
        # # Normalising to prevent explosion
        # du1=w.T@s_w@w
        # w/=(du1**0.5)

    thresh_lda=((w.T@mu_data_0)+(w.T@mu_data_1))/2
    
    y_lda_train_0=(w.T@train_data_0).squeeze(0)
    y_lda_train_1=(w.T@train_data_1).squeeze(0)
    y_lda_0=(w.T@test_data_0).squeeze(0)
    y_lda_1=(w.T@test_data_1).squeeze(0)
    
    train_res_lda0=[0 if y_lda_train_0[i]>=thresh_lda else 1 for i in range(y_lda_train_0.shape[0])]
    train_res_lda1=[0 if y_lda_train_1[i]>=thresh_lda else 1 for i in range(y_lda_train_1.shape[0])]
    test_res_lda0=[0 if y_lda_0[i]>=thresh_lda else 1 for i in range(y_lda_0.shape[0])]
    test_res_lda1=[0 if y_lda_1[i]>=thresh_lda else 1 for i in range(y_lda_1.shape[0])]
    
    print("Train Data:")
    print("Class 0 LDA Accuracy: ", sum(train_res_lda0)/y_lda_train_0.shape[0])
    print("Class 1 LDA Accuracy: ", 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]))
    print()
    print("Test Data:")
    print("Class 0 LDA Accuracy: ", sum(test_res_lda0)/y_lda_0.shape[0])
    print("Class 1 LDA Accuracy: ", 1-(sum(test_res_lda1)/y_lda_1.shape[0]))
    
    return w, [sum(train_res_lda0)/y_lda_train_0.shape[0], 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]), sum(test_res_lda0)/y_lda_0.shape[0], 1-(sum(test_res_lda1)/y_lda_1.shape[0])]


w_lda_grad_desc,_=lda_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, eta=1e-2, epochs=10000)


w_lda_closed_form,_=lda_closed_form(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)





def lda_l2_regularized_closed_form(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lambie):
    # Within-Class Scatter Matrix
    s_w =(train_data_0.shape[1]-1)*cov_data_0
    s_w+=((train_data_1.shape[1]-1)*cov_data_1)
    
    # Between Class Scatter Matrix
    mu_tot=(train_data_0.sum(axis=1)+train_data_1.sum(axis=1))/(train_data_0.shape[1]+train_data_1.shape[1])
    s_b =train_data_0.shape[1]*(mu_data_0-mu_tot)@(mu_data_0-mu_tot).T
    s_b+=train_data_1.shape[1]*(mu_data_1-mu_tot)@(mu_data_1-mu_tot).T

    eigie=np.linalg.eig(np.linalg.inv(s_w-lambie*np.eye(s_w.shape[0]))@s_b)
    # eigie_dum=np.linalg.eig(np.linalg.inv(s_w)@s_b)
    # print(eigie[0])
    print(eigie[1])
    # print("dum")
    # print(eigie_dum[0])
    # print(eigie_dum[1])
    
    thresh_lda=((eigie[1][:,0]@mu_data_0)+(eigie[1][:,0]@mu_data_1))/2
    
    y_lda_train_0=eigie[1][:,0]@train_data_0
    y_lda_train_1=eigie[1][:,0]@train_data_1
    y_lda_0=eigie[1][:,0]@test_data_0
    y_lda_1=eigie[1][:,0]@test_data_1
    
    train_res_lda0=[0 if y_lda_train_0[i]>=thresh_lda else 1 for i in range(y_lda_train_0.shape[0])]
    train_res_lda1=[0 if y_lda_train_1[i]>=thresh_lda else 1 for i in range(y_lda_train_1.shape[0])]
    test_res_lda0=[0 if y_lda_0[i]>=thresh_lda else 1 for i in range(y_lda_0.shape[0])]
    test_res_lda1=[0 if y_lda_1[i]>=thresh_lda else 1 for i in range(y_lda_1.shape[0])]

    print("Train Data:")
    print("Class 0 LDA L2 Accuracy: ", sum(train_res_lda0)/y_lda_train_0.shape[0])
    print("Class 1 LDA L2 Accuracy: ", 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]))
    print()
    print("Test Data:")
    print("Class 0 LDA L2 Accuracy: ", sum(test_res_lda0)/y_lda_0.shape[0])
    print("Class 1 LDA L2 Accuracy: ", 1-(sum(test_res_lda1)/y_lda_1.shape[0]))
    
    return eigie[1][:,0], [sum(train_res_lda0)/y_lda_train_0.shape[0], 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]), sum(test_res_lda0)/y_lda_0.shape[0], 1-(sum(test_res_lda1)/y_lda_1.shape[0])]


def lda_l2_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lambie, eta, epochs):
    # Within-Class Scatter Matrix
    s_w =(train_data_0.shape[1]-1)*cov_data_0
    s_w+=((train_data_1.shape[1]-1)*cov_data_1)
    
    # Between Class Scatter Matrix
    mu_tot=(train_data_0.sum(axis=1)+train_data_1.sum(axis=1))/(train_data_0.shape[1]+train_data_1.shape[1])
    s_b =train_data_0.shape[1]*(mu_data_0-mu_tot)@(mu_data_0-mu_tot).T
    s_b+=train_data_1.shape[1]*(mu_data_1-mu_tot)@(mu_data_1-mu_tot).T

    w=np.ones(s_w.shape[0]).reshape(-1,1)

    for i in range(epochs):
        du1=w.T@(s_w+lambie*np.eye(s_w.shape[0]))@w
        du2=w.T@s_b@w
        grad_w=(2*(-du2*(s_w+lambie*np.eye(s_w.shape[0]))+du1*s_b)@w)/(du1**2)
        w-=eta*grad_w
        # # Normalising to prevent explosion
        # du1=w.T@s_w@w
        # w/=(du1**0.5)

    thresh_lda=((w.T@mu_data_0)+(w.T@mu_data_1))/2
    
    y_lda_train_0=(w.T@train_data_0).squeeze(0)
    y_lda_train_1=(w.T@train_data_1).squeeze(0)
    y_lda_0=(w.T@test_data_0).squeeze(0)
    y_lda_1=(w.T@test_data_1).squeeze(0)
    
    train_res_lda0=[0 if y_lda_train_0[i]>=thresh_lda else 1 for i in range(y_lda_train_0.shape[0])]
    train_res_lda1=[0 if y_lda_train_1[i]>=thresh_lda else 1 for i in range(y_lda_train_1.shape[0])]
    test_res_lda0=[0 if y_lda_0[i]>=thresh_lda else 1 for i in range(y_lda_0.shape[0])]
    test_res_lda1=[0 if y_lda_1[i]>=thresh_lda else 1 for i in range(y_lda_1.shape[0])]
    
    print("Train Data:")
    print("Class 0 LDA L2 Accuracy: ", sum(train_res_lda0)/y_lda_train_0.shape[0])
    print("Class 1 LDA L2 Accuracy: ", 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]))
    print()
    print("Test Data:")
    print("Class 0 LDA L2 Accuracy: ", sum(test_res_lda0)/y_lda_0.shape[0])
    print("Class 1 LDA L2 Accuracy: ", 1-(sum(test_res_lda1)/y_lda_1.shape[0]))
    
    return w, [sum(train_res_lda0)/y_lda_train_0.shape[0], 1-(sum(train_res_lda1)/y_lda_train_1.shape[0]), sum(test_res_lda0)/y_lda_0.shape[0], 1-(sum(test_res_lda1)/y_lda_1.shape[0])]


lambie=1e3
w_lda_l2_grad_des,_c=lda_l2_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lambie, eta=1e-2, epochs=10000)


lambie=1e3
w_lda_l2_closed_form,_=lda_l2_regularized_closed_form(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lambie)




















print("Gradient Descent Weight:")
print(w_lda_grad_desc.squeeze(1))
print("Closed Form Weight:")
print(w_lda_closed_form)





print("Gradient Descent Weight:")
print(w_lda_l2_grad_desc.squeeze(1))
print("Closed Form Weight:")
print(w_lda_l2_closed_form)

















# Lets rerun the classifiers
train_data_0, test_data_0, train_data_1, test_data_1 = datagen(n_tot=500, p_class0=0.9, mean1=np.array([0,0]), mean2=np.array([1,3]), cov1=np.eye(2), cov2=np.eye(2), noise_bool=0, mean_noise=0, var_noise=1)


gaussian_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)


bernoulli_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)


w_lda_grad_desc=lda_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, eta=1e-2, epochs=10000)


lamb=1e3
w_lda_l2_grad_desc=lda_l2_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lamb, eta=1e-2, epochs=1000)





# train_gnb_0=[]
# train_gnb_1=[]
# train_bnb_0=[]
# train_bnb_1=[]
# train_lda_0=[]
# train_lda_1=[]
# train_lda_l2_0=[]
# train_lda_l2_1=[]

# test_gnb_0=[]
# test_gnb_1=[]
# test_bnb_0=[]
# test_bnb_1=[]
# test_lda_0=[]
# test_lda_1=[]
# test_lda_l2_0=[]
# test_lda_l2_1=[]

gnb=[]
bnb=[]
lda=[]
lda_l2=[]

# Gaussian noise (Low variance)
for i in range(5):
    print(f"Iteration: {i+1}")
    train_data_0, test_data_0, train_data_1, test_data_1 = datagen(n_tot=500, p_class0=0.5, mean1=np.array([0,0]), mean2=np.array([1,3]), cov1=np.eye(2), cov2=np.eye(2), noise_bool=1, mean_noise=0, var_noise=1)

    print()
    print("GNB")
    ag=gaussian_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)
    print()
    
    print("BNB")
    ab=bernoulli_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)
    print()

    print("LDA")
    _,al=lda_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, eta=1e-2, epochs=10000)
    print()

    print("LDA with L2")
    _,all2=lda_l2_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lamb, eta=1e-2, epochs=1000)
    print()
    
    gnb.append(ag)
    bnb.append(ab)
    lda.append(al)
    lda_l2.append(all2)
    

# Gaussian noise (High variance)
for i in range(5):
    print(f"Iteration: {6+i+1}")
    train_data_0, test_data_0, train_data_1, test_data_1 = datagen(n_tot=500, p_class0=0.5, mean1=np.array([0,0]), mean2=np.array([1,3]), cov1=np.eye(2), cov2=np.eye(2), noise_bool=1, mean_noise=0, var_noise=20**2)

    print()
    print("GNB")
    ag=gaussian_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)
    print()
    
    print("BNB")
    ab=bernoulli_naive_bayes(p_class0, train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1)
    print()

    print("LDA")
    _,al=lda_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, eta=1e-2, epochs=10000)
    print()

    print("LDA with L2")
    _,all2=lda_l2_grad_desc(train_data_0, test_data_0, train_data_1, test_data_1, mu_data_0, cov_data_0, mu_data_1, cov_data_1, lamb, eta=1e-2, epochs=1000)
    print()
    
    gnb.append(ag)
    bnb.append(ab)
    lda.append(al)
    lda_l2.append(all2)

gnb=np.array(gnb)
bnb=np.array(bnb)
lda=np.array(lda)
lda_l2=np.array(lda_l2)


# Train Data Class 0 (Low Variance)
plt.plot(np.arange(1,6), gnb[:5,0])
plt.plot(np.arange(1,6), bnb[:5,0])
plt.plot(np.arange(1,6), lda[:5,0])
plt.plot(np.arange(1,6), lda_l2[:5,0])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Train Data Class 0 (Low Variance)")
plt.show()

# Test Data Class 0 (Low Variance)
plt.plot(np.arange(1,6), gnb[:5,1])
plt.plot(np.arange(1,6), bnb[:5,1])
plt.plot(np.arange(1,6), lda[:5,1])
plt.plot(np.arange(1,6), lda_l2[:5,1])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Test Data Class 0 (Low Variance)")
plt.show()

# Train Data Class 1 (Low Variance)
plt.plot(np.arange(1,6), gnb[:5,2])
plt.plot(np.arange(1,6), bnb[:5,2])
plt.plot(np.arange(1,6), lda[:5,2])
plt.plot(np.arange(1,6), lda_l2[:5,2])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Train Data Class 1 (Low Variance)")
plt.show()

# Test Data Class 1 (Low Variance)
plt.plot(np.arange(1,6), gnb[:5,3])
plt.plot(np.arange(1,6), bnb[:5,3])
plt.plot(np.arange(1,6), lda[:5,3])
plt.plot(np.arange(1,6), lda_l2[:5,3])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Test Data Class 1 (Low Variance)")
plt.show()


# Train Data Class 0 (High Variance)
plt.plot(np.arange(6,11), gnb[5:,0])
plt.plot(np.arange(6,11), bnb[5:,0])
plt.plot(np.arange(6,11), lda[5:,0])
plt.plot(np.arange(6,11), lda_l2[5:,0])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Train Data Class 0 (High Variance)")
plt.show()

# Test Data Class 0 (High Variance)
plt.plot(np.arange(6,11), gnb[5:,1])
plt.plot(np.arange(6,11), bnb[5:,1])
plt.plot(np.arange(6,11), lda[5:,1])
plt.plot(np.arange(6,11), lda_l2[5:,1])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Test Data Class 0 (High Variance)")
plt.show()

# Train Data Class 1 (High Variance)
plt.plot(np.arange(6,11), gnb[5:,2])
plt.plot(np.arange(6,11), bnb[5:,2])
plt.plot(np.arange(6,11), lda[5:,2])
plt.plot(np.arange(6,11), lda_l2[5:,2])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Train Data Class 1 (High Variance)")
plt.show()

# Test Data Class 1 (High Variance)
plt.plot(np.arange(6,11), gnb[5:,3])
plt.plot(np.arange(6,11), bnb[5:,3])
plt.plot(np.arange(6,11), lda[5:,3])
plt.plot(np.arange(6,11), lda_l2[5:,3])
plt.legend(["GNB", "BNB", "LDA", "LDA with L2"], bbox_to_anchor=(1.35,1))
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Test Data Class 1 (High Variance)")
plt.show()












